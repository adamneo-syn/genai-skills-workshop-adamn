{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "project_id =\"qwiklabs-gcp-02-cf6490c204fb\""
      ],
      "metadata": {
        "id": "i8nqUmHxVaXN"
      },
      "id": "i8nqUmHxVaXN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "\n",
        "bq_client = bigquery.Client(project=project_id)"
      ],
      "metadata": {
        "id": "dQf5I5BGVYQ-"
      },
      "id": "dQf5I5BGVYQ-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DEFINING OUR MODEL"
      ],
      "metadata": {
        "id": "rngB1jIyWEaA"
      },
      "id": "rngB1jIyWEaA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Herem we declare the Vertex AI model that we will perform the text similarity searches with."
      ],
      "metadata": {
        "id": "l-xXPZtJmxRg"
      },
      "id": "l-xXPZtJmxRg"
    },
    {
      "cell_type": "code",
      "id": "ZA5TVVmPM51qsQcgZ4XInTof",
      "metadata": {
        "tags": [],
        "id": "ZA5TVVmPM51qsQcgZ4XInTof",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96b16486-8072-43d8-bdce-f6fef9870a74"
      },
      "source": [
        "# Creating the model that will connect to a Vertex AI Text Embedding model\n",
        "query = \"\"\"CREATE OR REPLACE MODEL `RAG.embedding_model`\n",
        "REMOTE WITH CONNECTION `us.rag_embeddings`\n",
        "OPTIONS (ENDPOINT = 'text-embedding-005');\n",
        "\"\"\"\n",
        "\n",
        "query_job = bq_client.query(query)\n",
        "query_job.result()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<google.cloud.bigquery.table._EmptyRowIterator at 0x7f5eb4069750>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CREATING THE RAW BQ TABLE"
      ],
      "metadata": {
        "id": "kW75403aWIh9"
      },
      "id": "kW75403aWIh9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now create our reference table based on the provided CSV file"
      ],
      "metadata": {
        "id": "xcO0JqRUm6xj"
      },
      "id": "xcO0JqRUm6xj"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = 'RAG'\n",
        "table_name = 'aurora_bay_faqs'\n",
        "\n",
        "bq_dataset = bq_client.dataset(dataset_name)\n",
        "bq_table = bq_dataset.table(table_name)\n",
        "\n",
        "external_location = 'gs://labs.roitraining.com/aurora-bay-faqs/aurora-bay-faqs.csv'\n",
        "\n",
        "schema = [\n",
        "    bigquery.SchemaField(\"question\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"answer\", \"STRING\")\n",
        "]\n",
        "\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "        schema=schema,\n",
        "        skip_leading_rows=1,\n",
        "        source_format=bigquery.SourceFormat.CSV,\n",
        "    )\n",
        "\n",
        "load_job = bq_client.load_table_from_uri(\n",
        "        external_location, bq_table, job_config=job_config\n",
        "    )\n",
        "\n",
        "load_job.result()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NZEfdGGRDLq",
        "outputId": "04a00bed-a808-4dd8-83c7-04258190cb5e"
      },
      "id": "8NZEfdGGRDLq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LoadJob<project=qwiklabs-gcp-02-cf6490c204fb, location=US, id=175d9d1c-3fd4-4535-b01e-12490dbd9d2d>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CREATING THE BQ TABLE WITH EMBEDDINGS"
      ],
      "metadata": {
        "id": "Rfx5uaDPcb08"
      },
      "id": "Rfx5uaDPcb08"
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"\n",
        "CREATE OR REPLACE TABLE `qwiklabs-gcp-02-cf6490c204fb.RAG.aurora_bay_faqswith_embeddings` AS\n",
        "SELECT *\n",
        "FROM ML.GENERATE_EMBEDDING(\n",
        "    MODEL `RAG.embedding_model`,\n",
        "    (SELECT question, answer, concat('Q: ',question, ' A: ',answer) AS content FROM `qwiklabs-gcp-02-cf6490c204fb.RAG.aurora_bay_faqs`)\n",
        ");\n",
        "\"\"\"\n",
        "query_job = bq_client.query(query)  # API request\n",
        "query_job.result()  # Waits for the query to complete"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLutfLkfcfKb",
        "outputId": "827bae17-023c-42b6-dff7-b0ed725b5cc2"
      },
      "id": "oLutfLkfcfKb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<google.cloud.bigquery.table._EmptyRowIterator at 0x7f5eb40f7700>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CREATING OUR MATCHING SETUP"
      ],
      "metadata": {
        "id": "dhu7ZjpKmJKz"
      },
      "id": "dhu7ZjpKmJKz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we use BQ's vector search to return the top K responses. We allow the user to define the desired K in the method."
      ],
      "metadata": {
        "id": "HDNoNPDXmNoL"
      },
      "id": "HDNoNPDXmNoL"
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_bq_search(client,question,k):\n",
        "  # Defining our base query\n",
        "  base_query = \"\"\"\n",
        "  SELECT base.content\n",
        "FROM VECTOR_SEARCH(\n",
        "  TABLE `RAG.aurora_bay_faqs_with_embeddings`, 'ml_generate_embedding_result',\n",
        "  (\n",
        "  SELECT text_embedding, content AS query\n",
        "  FROM ML.GENERATE_TEXT_EMBEDDING(\n",
        "  MODEL `RAG.embedding_model`,\n",
        "  (SELECT '{}' AS content))\n",
        "  ),\n",
        "  top_k => {}, options => '{{\"fraction_lists_to_search\": 0.01}}')\n",
        "  \"\"\"\n",
        "  # customised query to the user's question.\n",
        "  customised_query = base_query.format(question, k)\n",
        "\n",
        "  query_job = client.query(customised_query)\n",
        "  output = \"\"\n",
        "  for row in query_job:\n",
        "    output += row.content + '\\n'\n",
        "\n",
        "  return output"
      ],
      "metadata": {
        "id": "-MG-4XYsRDPR"
      },
      "id": "-MG-4XYsRDPR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perform_bq_search(bq_client,\"what is life?\",1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "2AxUVwUdiZ_k",
        "outputId": "f87dcdda-083f-4b49-895e-a7106060ef90"
      },
      "id": "2AxUVwUdiZ_k",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Q: What is the local tax rate for businesses? A: Business taxes include a 4% sales tax and a 2% local business tax on net profits, in addition to any applicable state taxes.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt_with_examples(question, examples):\n",
        "\n",
        "  prompt = f\"\"\"\n",
        "  Instructions: Answer the following question using the provided context.\n",
        "\n",
        "  Question: {question}\n",
        "\n",
        "  Context: {examples}\n",
        "  \"\"\"\n",
        "\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "_Y69K1peRDTa"
      },
      "id": "_Y69K1peRDTa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CREATING OUR RAG SETUP BY BRINGING EVERYTHING TOGETHER"
      ],
      "metadata": {
        "id": "eBKNHPGRmCMG"
      },
      "id": "eBKNHPGRmCMG"
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_rag(project,location,model,question,k):\n",
        "\n",
        "  from google import genai\n",
        "  from google.genai.types import GenerateContentConfig\n",
        "  from google.genai import types\n",
        "\n",
        "  client = genai.Client(\n",
        "      vertexai=True,\n",
        "      project=project,\n",
        "      location=location,\n",
        "  )\n",
        "\n",
        "  bq_client = bigquery.Client(project=project_id)\n",
        "\n",
        "  # Fixing our safety settings for simplicity, but this can always bet set as a parameter in the future.\n",
        "  safety_settings = [types.SafetySetting(\n",
        "      category=\"HARM_CATEGORY_HATE_SPEECH\",\n",
        "      threshold=\"BLOCK_LOW_AND_ABOVE\"\n",
        "    ),types.SafetySetting(\n",
        "      category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "      threshold=\"BLOCK_LOW_AND_ABOVE\"\n",
        "    ),types.SafetySetting(\n",
        "      category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "      threshold=\"BLOCK_LOW_AND_ABOVE\"\n",
        "    ),types.SafetySetting(\n",
        "      category=\"HARM_CATEGORY_HARASSMENT\",\n",
        "      threshold=\"BLOCK_LOW_AND_ABOVE\"\n",
        "    )]\n",
        "  # We also fix our config for simplicity\n",
        "  generated_config = types.GenerateContentConfig(\n",
        "        temperature = 0.5,\n",
        "        top_p = 0.95,\n",
        "        max_output_tokens = 8152,\n",
        "        response_modalities = [\"TEXT\"],\n",
        "        safety_settings = safety_settings\n",
        "  )\n",
        "\n",
        "  # Defining our closes matches from BQ\n",
        "  rag_matches = perform_bq_search(bq_client,question,k)\n",
        "\n",
        "  # Defining our prompt with the RAG outputs\n",
        "  rag_ammended_prompt = build_prompt_with_examples(question, rag_matches)\n",
        "\n",
        "  # Getting our response\n",
        "  response = client.models.generate_content(model=model, contents=rag_ammended_prompt)\n",
        "\n",
        "  # Printing the results\n",
        "  print(f\"Initial Query: {question}\")\n",
        "  print(f\"Prompt: {rag_ammended_prompt}\")\n",
        "  print(f\"Response: {response.text}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Iwf6irRChUxr"
      },
      "id": "Iwf6irRChUxr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we test our outputs by passing a question to the GEN AI model. Here, we can see the setup leveraging the examples provided by BQ"
      ],
      "metadata": {
        "id": "sQzDgXKelnFC"
      },
      "id": "sQzDgXKelnFC"
    },
    {
      "cell_type": "code",
      "source": [
        "location=\"us-central1\"\n",
        "model = \"gemini-2.5-pro-preview-05-06\"\n",
        "question = \"What is special about Aurora Bay?\"\n",
        "k=3\n",
        "\n",
        "apply_rag(project_id,location,model,question,k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGIRUTeZhlBo",
        "outputId": "958bd4d2-e555-44a3-f039-68349e7eefb0"
      },
      "id": "SGIRUTeZhlBo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Query: What is special about Aurora Bay?\n",
            "Prompt: \n",
            "  Instructions: Answer the following question using the provided context.\n",
            "\n",
            "  Question: What is special about Aurora Bay?\n",
            "\n",
            "  Context: Q: What is the average temperature range in Aurora Bay? A: Winters average between 10°F to 25°F, while summers are milder, around 50°F to 65°F. Temperatures can vary with coastal weather patterns.\n",
            "Q: What types of recreation are available in Aurora Bay? A: Popular activities include fishing, kayaking, hiking in the nearby forests, and northern lights viewing in the winter.\n",
            "Q: What is the population of Aurora Bay? A: Aurora Bay has a population of approximately 3,200 residents, although it can fluctuate seasonally due to temporary fishing and tourism workforces.\n",
            "\n",
            "  \n",
            "Response: Based on the provided context, what is special about Aurora Bay includes its popular recreational activities such as fishing, kayaking, hiking in nearby forests, and particularly, **northern lights viewing in the winter**.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bv3KJmG-kRbJ"
      },
      "id": "Bv3KJmG-kRbJ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "student-02-76f58729f297 (May 12, 2025, 3:36:23 PM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}